{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some parts of this code are based on the Python script:\n",
    "# https://github.com/pytorch/tutorials/blob/master/beginner_source/transfer_learning_tutorial.py\n",
    "# License: BSD\n",
    "\n",
    "import time\n",
    "import os\n",
    "import copy\n",
    "\n",
    "# PyTorch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "import torchvision\n",
    "from torchvision import datasets, transforms\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "# Pennylane\n",
    "import pennylane as qml\n",
    "from pennylane import numpy as np\n",
    "\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "# Plotting\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "import itertools\n",
    "\n",
    "# OpenMP: number of parallel threads.\n",
    "# os.environ[\"OMP_NUM_THREADS\"] = \"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Epoch [1/1], Step [100/782], Loss: 1.7140\n",
      "Epoch [1/1], Step [200/782], Loss: 1.5788\n",
      "Epoch [1/1], Step [300/782], Loss: 1.6066\n",
      "Epoch [1/1], Step [400/782], Loss: 1.4730\n",
      "Epoch [1/1], Step [500/782], Loss: 1.2113\n",
      "Epoch [1/1], Step [600/782], Loss: 1.2181\n",
      "Epoch [1/1], Step [700/782], Loss: 1.3376\n",
      "Accuracy on the test set: 56.32%\n"
     ]
    }
   ],
   "source": [
    "# Hyperparameters\n",
    "batch_size = 64\n",
    "learning_rate = 0.001\n",
    "num_epochs = 1\n",
    "\n",
    "# Data loading and preprocessing for CIFAR-10\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
    "])\n",
    "\n",
    "\n",
    "\n",
    "train_dataset = torchvision.datasets.CIFAR10(root='./data', train=True, transform=transform, download=True)\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "test_dataset = torchvision.datasets.CIFAR10(root='./data', train=False, transform=transform, download=True)\n",
    "test_loader = torch.utils.data.DataLoader(dataset=test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "\n",
    "\n",
    "class SimpleCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleCNN, self).__init__()\n",
    "        # in[N, 3, 32, 32] => out[N, 16, 16, 16]\n",
    "        self.conv1 = nn.Sequential(\n",
    "            nn.Conv2d(\n",
    "                in_channels=3,\n",
    "                out_channels=16,\n",
    "                kernel_size=5,\n",
    "                stride=1,\n",
    "                padding=2\n",
    "            ),\n",
    "            nn.ReLU(True),\n",
    "            nn.MaxPool2d(kernel_size=2)\n",
    "        )\n",
    "        # in[N, 16, 16, 16] => out[N, 32, 8, 8]\n",
    "        self.conv2 = nn.Sequential(\n",
    "            nn.Conv2d(16, 32, 5, 1, 2),\n",
    "            nn.ReLU(True),\n",
    "            nn.MaxPool2d(2)\n",
    "        )\n",
    "        # in[N, 32 * 8 * 8] => out[N, 128]\n",
    "        self.fc1 = nn.Sequential(\n",
    "            nn.Linear(32 * 8 * 8, 128),\n",
    "            nn.ReLU(True)\n",
    "        )\n",
    "        # in[N, 128] => out[N, 64]\n",
    "        self.fc2 = nn.Sequential(\n",
    "            nn.Linear(128, 64),\n",
    "            nn.ReLU(True)\n",
    "        )\n",
    "        # in[N, 64] => out[N, 10]\n",
    "        self.out = nn.Linear(64, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = x.view(x.size(0), -1) # [N, 32 * 8 * 8]\n",
    "        x = self.fc1(x)\n",
    "        x = self.fc2(x)\n",
    "        output = self.out(x)\n",
    "        return output\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Instantiate the model, move it to GPU, and set up loss function and optimizer\n",
    "model = SimpleCNN().to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(num_epochs):\n",
    "    for i, (images, labels) in enumerate(train_loader):\n",
    "        images, labels = images.to(device), labels.to(device)  # Move data to GPU\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(images)\n",
    "\n",
    "        # Compute loss\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        # Backward pass and optimization\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if (i+1) % 100 == 0:\n",
    "            print(f\"Epoch [{epoch+1}/{num_epochs}], Step [{i+1}/{len(train_loader)}], Loss: {loss.item():.4f}\")\n",
    "\n",
    "# Testing loop\n",
    "model.eval()\n",
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for images, labels in test_loader:\n",
    "        images, labels = images.to(device), labels.to(device)  # Move data to GPU\n",
    "        outputs = model(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "print(f\"Accuracy on the test set: {(100 * correct / total):.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# of NN parameters:  285226\n",
      "Required qubit number:  19\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# NN weights\n",
    "\n",
    "numpy_weights = {}\n",
    "nw_list = [] \n",
    "nw_list_normal = []\n",
    "for name, param in model.state_dict().items():\n",
    "    numpy_weights[name] = param.cpu().numpy()\n",
    "for i in numpy_weights:\n",
    "    nw_list.append(list(numpy_weights[i].flatten()))\n",
    "for i in nw_list:\n",
    "    for j in i:\n",
    "        nw_list_normal.append(j)\n",
    "print(\"# of NN parameters: \", len(nw_list_normal))\n",
    "n_qubits = int(np.ceil(np.log2(len(nw_list_normal))))\n",
    "print(\"Required qubit number: \", n_qubits)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# dev = qml.device(\"default.qubit\", wires=n_qubits)\n",
    "dev = qml.device(\"lightning.gpu\", wires=n_qubits, batch_obs=True)\n",
    "\n",
    "n_qubit = n_qubits\n",
    "\n",
    "def H_layer(nqubits):\n",
    "    \"\"\"Layer of single-qubit Hadamard gates.\n",
    "    \"\"\"\n",
    "    for idx in range(nqubits):\n",
    "        qml.Hadamard(wires=idx)\n",
    "\n",
    "def RY_layer(w):\n",
    "    \"\"\"Layer of parametrized qubit rotations around the y axis.\n",
    "    \"\"\"\n",
    "    for idx, element in enumerate(w):\n",
    "        qml.RY(element, wires=idx)\n",
    "\n",
    "def RZ_layer(w):\n",
    "    \"\"\"Layer of parametrized qubit rotations around the y axis.\n",
    "    \"\"\"\n",
    "    for idx, element in enumerate(w):\n",
    "        qml.RZ(element, wires=idx)\n",
    "        \n",
    "def entangling_layer(nqubits):\n",
    "    \"\"\"Layer of CNOTs followed by another shifted layer of CNOT.\n",
    "    \"\"\"\n",
    "    # In other words it should apply something like :\n",
    "    # CNOT  CNOT  CNOT  CNOT...  CNOT\n",
    "    #   CNOT  CNOT  CNOT...  CNOT\n",
    "    for i in range(0, nqubits - 1, 2):  # Loop over even indices: i=0,2,...N-2\n",
    "        qml.CNOT(wires=[i, i + 1])\n",
    "    for i in range(1, nqubits - 1, 2):  # Loop over odd indices:  i=1,3,...N-3\n",
    "        qml.CNOT(wires=[i, i + 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Some tool function definition ###########\n",
    "\n",
    "def probs_to_weights(probs_):\n",
    "    \n",
    "    new_state_dict = {}\n",
    "    data_iterator = probs_.view(-1)\n",
    "\n",
    "    for name, param in SimpleCNN().state_dict().items():\n",
    "        shape = param.shape\n",
    "        num_elements = param.numel()\n",
    "        chunk = data_iterator[:num_elements].reshape(shape)\n",
    "        new_state_dict[name] = chunk\n",
    "        data_iterator = data_iterator[num_elements:]\n",
    "        \n",
    "    return new_state_dict\n",
    "\n",
    "def generate_qubit_states_torch(n_qubit):\n",
    "    # Create a tensor of shape (2**n_qubit, n_qubit) with all possible combinations of 0 and 1\n",
    "    all_states = torch.cartesian_prod(*[torch.tensor([-1, 1]) for _ in range(n_qubit)])\n",
    "    return all_states\n",
    "\n",
    "####################\n",
    "\n",
    "\n",
    "# @qml.qnode(dev, diff_method=\"spsa\")\n",
    "# def quantum_net(q_weights_flat):\n",
    "#     \"\"\"\n",
    "#     The variational quantum circuit.\n",
    "#     \"\"\"\n",
    "\n",
    "#     # Reshape weights\n",
    "#     q_weights = q_weights_flat.reshape(q_depth, n_qubits)\n",
    "\n",
    "#     # Start from state |+> , unbiased w.r.t. |0> and |1>\n",
    "#     H_layer(n_qubits)\n",
    "#     # Repeated layer\n",
    "#     for i in range(q_depth):\n",
    "        \n",
    "#         # Parameterised layer\n",
    "#         if i%2 == 0:\n",
    "#             for y in range(n_qubits):\n",
    "#                 qml.RY(q_weights[i][y], wires=y)\n",
    "#         else:\n",
    "#             for z in range(n_qubits):\n",
    "#                 qml.RZ(q_weights[i][z], wires=z)\n",
    "\n",
    "#         # Control Z gates\n",
    "#         for y in range(n_qubits - 1):\n",
    "#             qml.CZ(wires=[y, y + 1])\n",
    "    \n",
    "    \n",
    "    \n",
    "#     probs_ = qml.probs(wires=list(range(n_qubits)))\n",
    "    \n",
    "#     return probs_\n",
    "\n",
    "@qml.qnode(dev, diff_method=\"spsa\")\n",
    "# @qml.qnode(dev, diff_method=\"parameter-shift\")\n",
    "\n",
    "def quantum_net(q_weights_flat):\n",
    "    \"\"\"\n",
    "    The variational quantum circuit.\n",
    "    \"\"\"\n",
    "    # Reshape weights\n",
    "    q_weights = q_weights_flat.reshape(q_depth, n_qubits)\n",
    "    H_layer(n_qubits)\n",
    "    # Repeated layer\n",
    "    for i in range(q_depth):\n",
    "        # Parameterised layer\n",
    "        if i%2 == 0:\n",
    "            for y in range(n_qubits):\n",
    "                qml.RY(q_weights[i][y], wires=y)\n",
    "        else:\n",
    "            for z in range(n_qubits):\n",
    "                qml.RZ(q_weights[i][z], wires=z)\n",
    "        for y in range(n_qubits - 1):\n",
    "            qml.CZ(wires=[y, y + 1])\n",
    "    \n",
    "\n",
    "    \n",
    "    # state_mag = qml.probs(wires=list(range(n_qubits)))\n",
    "\n",
    "    return qml.probs(wires=list(range(n_qubits)))#x_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class LewHybridNN(nn.Module):\n",
    "    \"\"\"\n",
    "    Torch module implementing full quantum net.\n",
    "    \"\"\"\n",
    "\n",
    "    class MappingModel(nn.Module):\n",
    "        def __init__(self, input_size, hidden_sizes, output_size):\n",
    "            super().__init__()\n",
    "            # Initialize layers: an input layer, multiple hidden layers, and an output layer\n",
    "            self.input_layer = nn.Linear(input_size, hidden_sizes[0])\n",
    "            self.hidden_layers = nn.ModuleList([nn.Linear(hidden_sizes[i], hidden_sizes[i+1]) for i in range(len(hidden_sizes)-1)])\n",
    "            self.output_layer = nn.Linear(hidden_sizes[-1], output_size)\n",
    "            \n",
    "        def forward(self, X):\n",
    "            X = X.type_as(self.input_layer.weight)\n",
    "            X = self.input_layer(X)\n",
    "            for hidden in self.hidden_layers:\n",
    "                X = hidden(X)\n",
    "            output = self.output_layer(X)\n",
    "            return output\n",
    "        \n",
    "    def __init__(self):\n",
    "\n",
    "        super().__init__()\n",
    "        self.q_params = nn.Parameter(q_delta * torch.randn(q_depth * n_qubits))\n",
    "        # self.simple_cnn = SimpleCNN()\n",
    "        self.MappingNetwork = self.MappingModel(n_qubit+1, [40, 200, 40], 1).to(device)\n",
    "\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Defining how tensors are supposed to move through the *dressed* quantum\n",
    "        net.\n",
    "        \"\"\"\n",
    "        device = x.device\n",
    "        self.q_params.requires_grad = True\n",
    "        \n",
    "        easy_scale_coeff = 2**(n_qubit-1)\n",
    "        gamma = 0.1\n",
    "        beta  = 0.8\n",
    "        alpha = 0.3\n",
    "            \n",
    "        probs_ = quantum_net(self.q_params)\n",
    "        probs_ = probs_[:len(nw_list_normal)]\n",
    "        x_ = torch.abs(probs_) ** 2\n",
    "        x_ = (beta*torch.tanh(gamma*easy_scale_coeff*x_))**(alpha) \n",
    "        x_ = x_ - torch.mean(x_)\n",
    "        x_.to(device)\n",
    "        \n",
    "        probs_ = x_ \n",
    "        # print(probs_)\n",
    "        \n",
    "        \n",
    "        # Generate qubit states using PyTorch\n",
    "        qubit_states_torch = generate_qubit_states_torch(n_qubit)[:len(nw_list_normal)]\n",
    "        qubit_states_torch = qubit_states_torch.to(device)\n",
    "\n",
    "        # Combine qubit states with probability values using PyTorch\n",
    "        # combined_data_torch = torch.cat((qubit_states_torch, probs_.unsqueeze(1)), dim=1)\n",
    "        # print(\"probs_:\", probs_)\n",
    "        # print(\"qubit_states_torch:\", qubit_states_torch)\n",
    "        combined_data_torch = torch.cat((qubit_states_torch, probs_.unsqueeze(1)), dim=1)\n",
    "        # print(\"combined_data_torch:\", combined_data_torch)\n",
    "        # input_size = combined_data_torch.size(1)\n",
    "\n",
    "        prob_val_post_processed = self.MappingNetwork(combined_data_torch)\n",
    "\n",
    "        state_dict = probs_to_weights(prob_val_post_processed)\n",
    "\n",
    "        ######## \n",
    "            \n",
    "        \n",
    "        dtype = torch.float32  # Ensure all tensors are of this type\n",
    "\n",
    "        # Convolution 1\n",
    "        weight = state_dict['conv1.0.weight'].to(device).type(dtype)\n",
    "        bias = state_dict['conv1.0.bias'].to(device).type(dtype)\n",
    "\n",
    "        \n",
    "        x = F.conv2d(x, weight, bias, stride=1, padding=2)\n",
    "        x = F.relu(x)\n",
    "        x = F.max_pool2d(x, kernel_size=2)\n",
    "\n",
    "        # Convolution 2\n",
    "        weight = state_dict['conv2.0.weight'].to(device).type(dtype)\n",
    "        bias = state_dict['conv2.0.bias'].to(device).type(dtype)\n",
    "        x = F.conv2d(x, weight, bias, stride=1, padding=2)\n",
    "        x = F.relu(x)\n",
    "        x = F.max_pool2d(x, kernel_size=2)\n",
    "\n",
    "        # Flatten\n",
    "        x = x.view(x.size(0), -1)\n",
    "\n",
    "        # Fully connected 1\n",
    "        weight = state_dict['fc1.0.weight'].to(device).type(dtype)\n",
    "        bias = state_dict['fc1.0.bias'].to(device).type(dtype)\n",
    "        x = F.linear(x, weight, bias)\n",
    "        x = F.relu(x)\n",
    "\n",
    "        # Fully connected 2\n",
    "        weight = state_dict['fc2.0.weight'].to(device).type(dtype)\n",
    "        bias = state_dict['fc2.0.bias'].to(device).type(dtype)\n",
    "        x = F.linear(x, weight, bias)\n",
    "        x = F.relu(x)\n",
    "\n",
    "        # Output layer\n",
    "        weight = state_dict['out.weight'].to(device).type(dtype)\n",
    "        bias = state_dict['out.bias'].to(device).type(dtype)\n",
    "        x = F.linear(x, weight, bias)\n",
    "\n",
    "    \n",
    "        return x #self.simple_cnn(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# # Define the neural network using nn.Module\n",
    "# class MappingModel(nn.Module):\n",
    "#     def __init__(self, input_size, hidden_sizes, output_size):\n",
    "#         super(MappingModel, self).__init__()\n",
    "#         # Initialize layers: an input layer, multiple hidden layers, and an output layer\n",
    "#         self.input_layer = nn.Linear(input_size, hidden_sizes[0])\n",
    "#         self.batch_norms_input = nn.BatchNorm1d(hidden_sizes[0])\n",
    "#         self.hidden_layers = nn.ModuleList()\n",
    "#         self.batch_norms = nn.ModuleList()\n",
    "\n",
    "#         for i in range(len(hidden_sizes) - 1):\n",
    "#             self.hidden_layers.append(nn.Linear(hidden_sizes[i], hidden_sizes[i+1]))\n",
    "#             self.batch_norms.append(nn.BatchNorm1d(hidden_sizes[i+1]))\n",
    "\n",
    "#         self.output_layer = nn.Linear(hidden_sizes[-1], output_size)\n",
    "        \n",
    "#     def forward(self, X):\n",
    "#         # Ensure the input tensor is the same type as the weights\n",
    "#         X = X.type_as(self.input_layer.weight)\n",
    "\n",
    "#         # Input layer with batch normalization and ReLU activation\n",
    "#         X = self.input_layer(X)\n",
    "#         X = self.batch_norms_input(X)\n",
    "#         # X = F.relu(X)\n",
    "\n",
    "#         # Hidden layers with batch normalization and ReLU activation\n",
    "#         for hidden, batch_norm in zip(self.hidden_layers, self.batch_norms):\n",
    "#             X = hidden(X)\n",
    "#             X = batch_norm(X)\n",
    "#             # X = F.relu(X)\n",
    "\n",
    "#         # Output layer with linear activation\n",
    "#         output = self.output_layer(X)\n",
    "        \n",
    "#         return output\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# class LewHybridNN(nn.Module):\n",
    "#     \"\"\"\n",
    "#     Torch module implementing full quantum net.\n",
    "#     \"\"\"\n",
    "\n",
    "#     def __init__(self):\n",
    "\n",
    "#         super().__init__()\n",
    "#         self.q_params = nn.Parameter(q_delta * torch.randn(q_depth * n_qubits))\n",
    "#         # self.simple_cnn = SimpleCNN()\n",
    "#         self.MappingNetwork = MappingModel(n_qubit+1, [150, 50, 30, 20], 1)\n",
    "\n",
    "    \n",
    "#     def forward(self, x):\n",
    "#         \"\"\"\n",
    "#         Defining how tensors are supposed to move through the *dressed* quantum\n",
    "#         net.\n",
    "#         \"\"\"\n",
    "#         device = x.device\n",
    "#         self.q_params.requires_grad = True\n",
    "\n",
    "            \n",
    "#         probs_ = quantum_net(self.q_params)\n",
    "#         probs_ = probs_[:len(nw_list_normal)]\n",
    "#         # print(probs_)\n",
    "#         # Generate qubit states using PyTorch\n",
    "#         qubit_states_torch = generate_qubit_states_torch(n_qubit)[:len(nw_list_normal)]\n",
    "#         qubit_states_torch = qubit_states_torch.to(device)\n",
    "\n",
    "#         # Combine qubit states with probability values using PyTorch\n",
    "#         combined_data_torch = torch.cat((qubit_states_torch, probs_.unsqueeze(1)), dim=1)\n",
    "#         # input_size = combined_data_torch.size(1)\n",
    "\n",
    "#         self.MappingNetwork.to(device)\n",
    "#         prob_val_post_processed = self.MappingNetwork(combined_data_torch)\n",
    "\n",
    "#         state_dict = probs_to_weights(prob_val_post_processed)\n",
    "\n",
    "#         ######## \n",
    "            \n",
    "        \n",
    "#         dtype = torch.float32  # Ensure all tensors are of this type\n",
    "\n",
    "#         # Convolution 1\n",
    "#         weight = state_dict['conv1.0.weight'].to(device).type(dtype)\n",
    "#         bias = state_dict['conv1.0.bias'].to(device).type(dtype)\n",
    "\n",
    "        \n",
    "#         x = F.conv2d(x, weight, bias, stride=1, padding=2)\n",
    "#         x = F.relu(x)\n",
    "#         x = F.max_pool2d(x, kernel_size=2)\n",
    "\n",
    "#         # Convolution 2\n",
    "#         weight = state_dict['conv2.0.weight'].to(device).type(dtype)\n",
    "#         bias = state_dict['conv2.0.bias'].to(device).type(dtype)\n",
    "#         x = F.conv2d(x, weight, bias, stride=1, padding=2)\n",
    "#         x = F.relu(x)\n",
    "#         x = F.max_pool2d(x, kernel_size=2)\n",
    "\n",
    "#         # Flatten\n",
    "#         x = x.view(x.size(0), -1)\n",
    "\n",
    "#         # Fully connected 1\n",
    "#         weight = state_dict['fc1.0.weight'].to(device).type(dtype)\n",
    "#         bias = state_dict['fc1.0.bias'].to(device).type(dtype)\n",
    "#         x = F.linear(x, weight, bias)\n",
    "#         x = F.relu(x)\n",
    "\n",
    "#         # Fully connected 2\n",
    "#         weight = state_dict['fc2.0.weight'].to(device).type(dtype)\n",
    "#         bias = state_dict['fc2.0.bias'].to(device).type(dtype)\n",
    "#         x = F.linear(x, weight, bias)\n",
    "#         x = F.relu(x)\n",
    "\n",
    "#         # Output layer\n",
    "#         weight = state_dict['out.weight'].to(device).type(dtype)\n",
    "#         bias = state_dict['out.bias'].to(device).type(dtype)\n",
    "#         x = F.linear(x, weight, bias)\n",
    "\n",
    "    \n",
    "#         return x #self.simple_cnn(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# of trainable parameter in Mapping model:  17121\n",
      "# of trainable parameter in QNN model:  950\n",
      "# of trainable parameter in full model:  18071\n"
     ]
    }
   ],
   "source": [
    "step = 0.0004               # Learning rate\n",
    "batch_size = 64             # Number of samples for each training step\n",
    "num_epochs = 10             # Number of training epochs\n",
    "q_depth = 50                 # Depth of the quantum circuit (number of variational layers)\n",
    "gamma_lr_scheduler = 0.1    # Learning rate reduction applied every 10 epochs.\n",
    "q_delta = 0.01              # Initial spread of random quantum weights\n",
    "\n",
    "\n",
    "\n",
    "# Instantiate the model, move it to GPU, and set up loss function and optimizer\n",
    "model = LewHybridNN().to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=step)\n",
    "\n",
    "# exp_lr_scheduler = lr_scheduler.StepLR(\n",
    "#     optimizer, step_size=10, gamma=gamma_lr_scheduler\n",
    "# )\n",
    "\n",
    "\n",
    "num_trainable_params_QNN = sum(p.numel() for p in LewHybridNN.MappingModel(n_qubit+1,  [40, 200, 40], 1).parameters() if p.requires_grad)\n",
    "\n",
    "num_trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(\"# of trainable parameter in Mapping model: \", num_trainable_params_QNN)\n",
    "print(\"# of trainable parameter in QNN model: \", num_trainable_params - num_trainable_params_QNN)\n",
    "print(\"# of trainable parameter in full model: \", num_trainable_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10], Step [1/782], Loss: 78.0615, batch time: 7.080328941345215\n",
      "Epoch [1/10], Step [2/782], Loss: 2.3047, batch time: 6.704021692276001\n",
      "Epoch [1/10], Step [3/782], Loss: 2.3002, batch time: 6.758169412612915\n",
      "Epoch [1/10], Step [4/782], Loss: 2.2994, batch time: 6.705949544906616\n",
      "Epoch [1/10], Step [5/782], Loss: 2.3043, batch time: 6.787644624710083\n",
      "Epoch [1/10], Step [6/782], Loss: 2.3020, batch time: 6.7213218212127686\n",
      "Epoch [1/10], Step [7/782], Loss: 2.2947, batch time: 6.813401222229004\n",
      "Epoch [1/10], Step [8/782], Loss: 2.3051, batch time: 6.7331461906433105\n",
      "Epoch [1/10], Step [9/782], Loss: 2.3060, batch time: 6.783542633056641\n",
      "Epoch [1/10], Step [10/782], Loss: 2.3049, batch time: 6.798852920532227\n",
      "Epoch [1/10], Step [11/782], Loss: 2.2987, batch time: 6.725883960723877\n",
      "Epoch [1/10], Step [12/782], Loss: 2.3036, batch time: 6.743467569351196\n",
      "Epoch [1/10], Step [13/782], Loss: 2.3085, batch time: 6.7090160846710205\n",
      "Epoch [1/10], Step [14/782], Loss: 2.3030, batch time: 6.799898386001587\n",
      "Epoch [1/10], Step [15/782], Loss: 2.3084, batch time: 6.739892959594727\n",
      "Epoch [1/10], Step [16/782], Loss: 2.3026, batch time: 6.762547731399536\n",
      "Epoch [1/10], Step [17/782], Loss: 2.2900, batch time: 6.726613998413086\n",
      "Epoch [1/10], Step [18/782], Loss: 2.3002, batch time: 6.755052328109741\n",
      "Epoch [1/10], Step [19/782], Loss: 2.3049, batch time: 6.757429122924805\n",
      "Epoch [1/10], Step [20/782], Loss: 2.3094, batch time: 6.713940858840942\n",
      "Epoch [1/10], Step [21/782], Loss: 2.2986, batch time: 6.796843528747559\n",
      "Epoch [1/10], Step [22/782], Loss: 2.3035, batch time: 6.734679222106934\n",
      "Epoch [1/10], Step [23/782], Loss: 2.3066, batch time: 6.830057382583618\n",
      "Epoch [1/10], Step [24/782], Loss: 2.3030, batch time: 6.691313982009888\n",
      "Epoch [1/10], Step [25/782], Loss: 2.3011, batch time: 6.8231377601623535\n",
      "Epoch [1/10], Step [26/782], Loss: 2.3066, batch time: 6.735297918319702\n",
      "Epoch [1/10], Step [27/782], Loss: 2.3006, batch time: 6.772306680679321\n",
      "Epoch [1/10], Step [28/782], Loss: 2.3025, batch time: 6.765278339385986\n",
      "Epoch [1/10], Step [29/782], Loss: 2.2990, batch time: 6.748944282531738\n",
      "Epoch [1/10], Step [30/782], Loss: 2.3035, batch time: 6.771597862243652\n",
      "Epoch [1/10], Step [31/782], Loss: 2.2998, batch time: 6.712263822555542\n",
      "Epoch [1/10], Step [32/782], Loss: 2.3079, batch time: 6.784343242645264\n",
      "Epoch [1/10], Step [33/782], Loss: 2.3050, batch time: 6.720334768295288\n",
      "Epoch [1/10], Step [34/782], Loss: 2.3036, batch time: 6.773087739944458\n",
      "Epoch [1/10], Step [35/782], Loss: 2.3026, batch time: 6.808820009231567\n",
      "Epoch [1/10], Step [36/782], Loss: 2.3038, batch time: 6.7122557163238525\n",
      "Epoch [1/10], Step [37/782], Loss: 2.3009, batch time: 6.772540807723999\n",
      "Epoch [1/10], Step [38/782], Loss: 2.3018, batch time: 6.734501361846924\n",
      "Epoch [1/10], Step [39/782], Loss: 2.3039, batch time: 6.770199537277222\n",
      "Epoch [1/10], Step [40/782], Loss: 2.3113, batch time: 6.760369062423706\n",
      "Epoch [1/10], Step [41/782], Loss: 2.3056, batch time: 6.840563058853149\n",
      "Epoch [1/10], Step [42/782], Loss: 2.3071, batch time: 6.753576040267944\n",
      "Epoch [1/10], Step [43/782], Loss: 2.2986, batch time: 6.793949127197266\n",
      "Epoch [1/10], Step [44/782], Loss: 2.3045, batch time: 6.80039381980896\n",
      "Epoch [1/10], Step [45/782], Loss: 2.2965, batch time: 6.770869255065918\n",
      "Epoch [1/10], Step [46/782], Loss: 2.3041, batch time: 6.823690891265869\n",
      "Epoch [1/10], Step [47/782], Loss: 2.2972, batch time: 6.7212114334106445\n",
      "Epoch [1/10], Step [48/782], Loss: 2.3041, batch time: 6.778322458267212\n",
      "Epoch [1/10], Step [49/782], Loss: 2.2950, batch time: 6.748866319656372\n",
      "Epoch [1/10], Step [50/782], Loss: 2.3077, batch time: 6.79001784324646\n",
      "Epoch [1/10], Step [51/782], Loss: 2.3124, batch time: 6.737164735794067\n",
      "Epoch [1/10], Step [52/782], Loss: 2.2953, batch time: 6.823533773422241\n",
      "Epoch [1/10], Step [53/782], Loss: 2.2973, batch time: 6.783107042312622\n",
      "Epoch [1/10], Step [54/782], Loss: 2.3100, batch time: 6.723861217498779\n",
      "Epoch [1/10], Step [55/782], Loss: 2.3061, batch time: 6.886485576629639\n",
      "Epoch [1/10], Step [56/782], Loss: 2.3061, batch time: 6.728736639022827\n",
      "Epoch [1/10], Step [57/782], Loss: 2.3028, batch time: 6.833407402038574\n",
      "Epoch [1/10], Step [58/782], Loss: 2.2988, batch time: 6.714668035507202\n",
      "Epoch [1/10], Step [59/782], Loss: 2.3084, batch time: 6.818582534790039\n",
      "Epoch [1/10], Step [60/782], Loss: 2.2971, batch time: 6.8212456703186035\n",
      "Epoch [1/10], Step [61/782], Loss: 2.3032, batch time: 6.739834785461426\n",
      "Epoch [1/10], Step [62/782], Loss: 2.3094, batch time: 6.78971004486084\n",
      "Epoch [1/10], Step [63/782], Loss: 2.3073, batch time: 6.732308387756348\n",
      "Epoch [1/10], Step [64/782], Loss: 2.3053, batch time: 6.826671123504639\n",
      "Epoch [1/10], Step [65/782], Loss: 2.3119, batch time: 6.724893808364868\n",
      "Epoch [1/10], Step [66/782], Loss: 2.3038, batch time: 6.790395021438599\n",
      "Epoch [1/10], Step [67/782], Loss: 2.2998, batch time: 6.873165130615234\n",
      "Epoch [1/10], Step [68/782], Loss: 2.2977, batch time: 6.9159770011901855\n",
      "Epoch [1/10], Step [69/782], Loss: 2.3049, batch time: 6.836650609970093\n",
      "Epoch [1/10], Step [70/782], Loss: 2.3060, batch time: 6.762109994888306\n",
      "Epoch [1/10], Step [71/782], Loss: 2.3140, batch time: 6.808871746063232\n",
      "Epoch [1/10], Step [72/782], Loss: 2.3091, batch time: 6.745596408843994\n",
      "Epoch [1/10], Step [73/782], Loss: 2.3085, batch time: 6.8255455493927\n",
      "Epoch [1/10], Step [74/782], Loss: 2.3051, batch time: 6.731682062149048\n",
      "Epoch [1/10], Step [75/782], Loss: 2.2987, batch time: 6.788981199264526\n",
      "Epoch [1/10], Step [76/782], Loss: 2.3061, batch time: 6.7039573192596436\n",
      "Epoch [1/10], Step [77/782], Loss: 2.2987, batch time: 6.811159610748291\n",
      "Epoch [1/10], Step [78/782], Loss: 2.3041, batch time: 6.7936482429504395\n",
      "Epoch [1/10], Step [79/782], Loss: 2.2997, batch time: 6.727441310882568\n",
      "Epoch [1/10], Step [80/782], Loss: 2.3115, batch time: 6.778840780258179\n",
      "Epoch [1/10], Step [81/782], Loss: 2.3024, batch time: 6.702183961868286\n",
      "Epoch [1/10], Step [82/782], Loss: 2.3050, batch time: 6.80771017074585\n",
      "Epoch [1/10], Step [83/782], Loss: 2.3035, batch time: 6.708415269851685\n",
      "Epoch [1/10], Step [84/782], Loss: 2.3033, batch time: 6.83940315246582\n",
      "Epoch [1/10], Step [85/782], Loss: 2.3044, batch time: 6.8593995571136475\n",
      "Epoch [1/10], Step [86/782], Loss: 2.3050, batch time: 6.7791502475738525\n",
      "Epoch [1/10], Step [87/782], Loss: 2.3041, batch time: 6.81920051574707\n",
      "Epoch [1/10], Step [88/782], Loss: 2.2977, batch time: 6.716597557067871\n",
      "Epoch [1/10], Step [89/782], Loss: 2.3021, batch time: 6.824172735214233\n",
      "Epoch [1/10], Step [90/782], Loss: 2.3019, batch time: 6.870639085769653\n",
      "Epoch [1/10], Step [91/782], Loss: 2.3000, batch time: 6.876925706863403\n",
      "Epoch [1/10], Step [92/782], Loss: 2.3030, batch time: 6.722342014312744\n",
      "Epoch [1/10], Step [93/782], Loss: 2.2970, batch time: 6.870312452316284\n",
      "Epoch [1/10], Step [94/782], Loss: 2.2948, batch time: 7.195266008377075\n",
      "Epoch [1/10], Step [95/782], Loss: 2.3085, batch time: 7.0522871017456055\n",
      "Epoch [1/10], Step [96/782], Loss: 2.3091, batch time: 7.13966703414917\n",
      "Epoch [1/10], Step [97/782], Loss: 2.3059, batch time: 6.936594009399414\n",
      "Epoch [1/10], Step [98/782], Loss: 2.3003, batch time: 7.099213600158691\n",
      "Epoch [1/10], Step [99/782], Loss: 2.3009, batch time: 7.002595901489258\n",
      "Epoch [1/10], Step [100/782], Loss: 2.3091, batch time: 7.1847028732299805\n",
      "Epoch [1/10], Step [101/782], Loss: 2.3115, batch time: 6.943516969680786\n",
      "Epoch [1/10], Step [102/782], Loss: 2.2949, batch time: 7.076909065246582\n",
      "Epoch [1/10], Step [103/782], Loss: 2.3052, batch time: 7.204421520233154\n",
      "Epoch [1/10], Step [104/782], Loss: 2.3053, batch time: 6.924685001373291\n",
      "Epoch [1/10], Step [105/782], Loss: 2.2980, batch time: 6.98805832862854\n",
      "Epoch [1/10], Step [106/782], Loss: 2.3064, batch time: 7.0043134689331055\n",
      "Epoch [1/10], Step [107/782], Loss: 2.3097, batch time: 7.092780828475952\n",
      "Epoch [1/10], Step [108/782], Loss: 2.3083, batch time: 6.960661172866821\n",
      "Epoch [1/10], Step [109/782], Loss: 2.3115, batch time: 6.99029278755188\n",
      "Epoch [1/10], Step [110/782], Loss: 2.3109, batch time: 6.989616394042969\n",
      "Epoch [1/10], Step [111/782], Loss: 2.2945, batch time: 6.943742990493774\n",
      "Epoch [1/10], Step [112/782], Loss: 2.2975, batch time: 7.122592449188232\n",
      "Epoch [1/10], Step [113/782], Loss: 2.3024, batch time: 7.022343158721924\n",
      "Epoch [1/10], Step [114/782], Loss: 2.3007, batch time: 7.038412570953369\n",
      "Epoch [1/10], Step [115/782], Loss: 2.3044, batch time: 7.094512939453125\n",
      "Epoch [1/10], Step [116/782], Loss: 2.3041, batch time: 7.043869256973267\n",
      "Epoch [1/10], Step [117/782], Loss: 2.3039, batch time: 6.944648027420044\n",
      "Epoch [1/10], Step [118/782], Loss: 2.3012, batch time: 6.963402271270752\n",
      "Epoch [1/10], Step [119/782], Loss: 2.3018, batch time: 7.119678258895874\n",
      "Epoch [1/10], Step [120/782], Loss: 2.2966, batch time: 7.072455406188965\n",
      "Epoch [1/10], Step [121/782], Loss: 2.2901, batch time: 7.079957723617554\n",
      "Epoch [1/10], Step [122/782], Loss: 2.2917, batch time: 6.992034912109375\n",
      "Epoch [1/10], Step [123/782], Loss: 2.3070, batch time: 7.166008949279785\n",
      "Epoch [1/10], Step [124/782], Loss: 2.3020, batch time: 6.987295627593994\n",
      "Epoch [1/10], Step [125/782], Loss: 2.3042, batch time: 7.005025148391724\n",
      "Epoch [1/10], Step [126/782], Loss: 2.3056, batch time: 6.955747365951538\n",
      "Epoch [1/10], Step [127/782], Loss: 2.3035, batch time: 6.993089437484741\n",
      "Epoch [1/10], Step [128/782], Loss: 2.3062, batch time: 7.063314437866211\n",
      "Epoch [1/10], Step [129/782], Loss: 2.3058, batch time: 6.9950103759765625\n",
      "Epoch [1/10], Step [130/782], Loss: 2.3000, batch time: 7.059909105300903\n",
      "Epoch [1/10], Step [131/782], Loss: 2.3057, batch time: 6.9321277141571045\n",
      "Epoch [1/10], Step [132/782], Loss: 2.3076, batch time: 7.07184100151062\n",
      "Epoch [1/10], Step [133/782], Loss: 2.3030, batch time: 7.0898096561431885\n",
      "Epoch [1/10], Step [134/782], Loss: 2.3078, batch time: 7.102802991867065\n",
      "Epoch [1/10], Step [135/782], Loss: 2.3086, batch time: 7.01294732093811\n",
      "Epoch [1/10], Step [136/782], Loss: 2.3110, batch time: 6.916871070861816\n",
      "Epoch [1/10], Step [137/782], Loss: 2.2966, batch time: 7.091250419616699\n",
      "Epoch [1/10], Step [138/782], Loss: 2.3015, batch time: 7.139267206192017\n",
      "Epoch [1/10], Step [139/782], Loss: 2.3061, batch time: 7.082480430603027\n",
      "Epoch [1/10], Step [140/782], Loss: 2.3013, batch time: 7.04545259475708\n",
      "Epoch [1/10], Step [141/782], Loss: 2.2965, batch time: 7.14944314956665\n",
      "Epoch [1/10], Step [142/782], Loss: 2.3080, batch time: 6.903394937515259\n",
      "Epoch [1/10], Step [143/782], Loss: 2.2994, batch time: 7.143568754196167\n",
      "Epoch [1/10], Step [144/782], Loss: 2.2919, batch time: 7.222029209136963\n",
      "Epoch [1/10], Step [145/782], Loss: 2.2997, batch time: 7.022471189498901\n",
      "Epoch [1/10], Step [146/782], Loss: 2.3123, batch time: 7.109052658081055\n",
      "Epoch [1/10], Step [147/782], Loss: 2.2983, batch time: 6.953672409057617\n",
      "Epoch [1/10], Step [148/782], Loss: 2.3107, batch time: 7.234500885009766\n",
      "Epoch [1/10], Step [149/782], Loss: 2.2945, batch time: 7.031765460968018\n",
      "Epoch [1/10], Step [150/782], Loss: 2.3124, batch time: 7.008479833602905\n",
      "Epoch [1/10], Step [151/782], Loss: 2.2993, batch time: 7.119600296020508\n",
      "Epoch [1/10], Step [152/782], Loss: 2.3108, batch time: 7.010079860687256\n",
      "Epoch [1/10], Step [153/782], Loss: 2.3130, batch time: 7.225608825683594\n",
      "Epoch [1/10], Step [154/782], Loss: 2.3078, batch time: 6.9941630363464355\n",
      "Epoch [1/10], Step [155/782], Loss: 2.3053, batch time: 7.207714557647705\n",
      "Epoch [1/10], Step [156/782], Loss: 2.3060, batch time: 7.120198011398315\n",
      "Epoch [1/10], Step [157/782], Loss: 2.3042, batch time: 7.045516014099121\n",
      "Epoch [1/10], Step [158/782], Loss: 2.3100, batch time: 6.958602428436279\n",
      "Epoch [1/10], Step [159/782], Loss: 2.3015, batch time: 7.009490966796875\n",
      "Epoch [1/10], Step [160/782], Loss: 2.3106, batch time: 7.207024574279785\n",
      "Epoch [1/10], Step [161/782], Loss: 2.2927, batch time: 7.034098386764526\n",
      "Epoch [1/10], Step [162/782], Loss: 2.3086, batch time: 7.0385661125183105\n",
      "Epoch [1/10], Step [163/782], Loss: 2.3016, batch time: 7.021846532821655\n",
      "Epoch [1/10], Step [164/782], Loss: 2.3077, batch time: 7.118254661560059\n",
      "Epoch [1/10], Step [165/782], Loss: 2.3080, batch time: 6.979547500610352\n",
      "Epoch [1/10], Step [166/782], Loss: 2.2983, batch time: 7.065591812133789\n",
      "Epoch [1/10], Step [167/782], Loss: 2.3023, batch time: 6.94680118560791\n",
      "Epoch [1/10], Step [168/782], Loss: 2.3097, batch time: 7.139419317245483\n",
      "Epoch [1/10], Step [169/782], Loss: 2.2965, batch time: 7.154845714569092\n",
      "Epoch [1/10], Step [170/782], Loss: 2.3042, batch time: 6.983991622924805\n",
      "Epoch [1/10], Step [171/782], Loss: 2.3012, batch time: 7.132469177246094\n",
      "Epoch [1/10], Step [172/782], Loss: 2.3094, batch time: 7.048584461212158\n",
      "Epoch [1/10], Step [173/782], Loss: 2.3009, batch time: 7.155893564224243\n",
      "Epoch [1/10], Step [174/782], Loss: 2.2989, batch time: 6.9592180252075195\n",
      "Epoch [1/10], Step [175/782], Loss: 2.3048, batch time: 7.1260058879852295\n",
      "Epoch [1/10], Step [176/782], Loss: 2.3039, batch time: 7.057070970535278\n",
      "Epoch [1/10], Step [177/782], Loss: 2.2989, batch time: 7.205687046051025\n",
      "Epoch [1/10], Step [178/782], Loss: 2.2996, batch time: 7.027490139007568\n",
      "Epoch [1/10], Step [179/782], Loss: 2.3013, batch time: 7.066492795944214\n",
      "Epoch [1/10], Step [180/782], Loss: 2.3128, batch time: 6.7957117557525635\n",
      "Epoch [1/10], Step [181/782], Loss: 2.3070, batch time: 6.7476747035980225\n",
      "Epoch [1/10], Step [182/782], Loss: 2.3010, batch time: 6.810608386993408\n",
      "Epoch [1/10], Step [183/782], Loss: 2.3032, batch time: 6.745556116104126\n",
      "Epoch [1/10], Step [184/782], Loss: 2.3058, batch time: 6.809739351272583\n",
      "Epoch [1/10], Step [185/782], Loss: 2.2994, batch time: 6.807331800460815\n",
      "Epoch [1/10], Step [186/782], Loss: 2.2967, batch time: 6.710721731185913\n",
      "Epoch [1/10], Step [187/782], Loss: 2.3012, batch time: 6.869173765182495\n",
      "Epoch [1/10], Step [188/782], Loss: 2.3042, batch time: 6.711277484893799\n",
      "Epoch [1/10], Step [189/782], Loss: 2.3045, batch time: 6.776235818862915\n",
      "Epoch [1/10], Step [190/782], Loss: 2.3070, batch time: 6.712013244628906\n",
      "Epoch [1/10], Step [191/782], Loss: 2.3021, batch time: 6.770509719848633\n",
      "Epoch [1/10], Step [192/782], Loss: 2.2971, batch time: 6.690014600753784\n",
      "Epoch [1/10], Step [193/782], Loss: 2.3012, batch time: 6.76727819442749\n",
      "Epoch [1/10], Step [194/782], Loss: 2.3015, batch time: 6.774601221084595\n",
      "Epoch [1/10], Step [195/782], Loss: 2.3018, batch time: 6.803050756454468\n",
      "Epoch [1/10], Step [196/782], Loss: 2.3036, batch time: 6.781181812286377\n",
      "Epoch [1/10], Step [197/782], Loss: 2.3019, batch time: 6.756150960922241\n",
      "Epoch [1/10], Step [198/782], Loss: 2.2990, batch time: 6.8392438888549805\n",
      "Epoch [1/10], Step [199/782], Loss: 2.3067, batch time: 6.713865518569946\n",
      "Epoch [1/10], Step [200/782], Loss: 2.3062, batch time: 6.783219337463379\n",
      "Epoch [1/10], Step [201/782], Loss: 2.3068, batch time: 6.732020616531372\n",
      "Epoch [1/10], Step [202/782], Loss: 2.3016, batch time: 6.777316331863403\n",
      "Epoch [1/10], Step [203/782], Loss: 2.3057, batch time: 6.8077356815338135\n",
      "Epoch [1/10], Step [204/782], Loss: 2.2910, batch time: 6.735546350479126\n",
      "Epoch [1/10], Step [205/782], Loss: 2.2957, batch time: 6.897671222686768\n",
      "Epoch [1/10], Step [206/782], Loss: 2.3046, batch time: 6.766942024230957\n",
      "Epoch [1/10], Step [207/782], Loss: 2.2945, batch time: 6.821773290634155\n",
      "Epoch [1/10], Step [208/782], Loss: 2.3054, batch time: 6.755466938018799\n",
      "Epoch [1/10], Step [209/782], Loss: 2.3096, batch time: 6.849214553833008\n",
      "Epoch [1/10], Step [210/782], Loss: 2.3016, batch time: 6.798173904418945\n",
      "Epoch [1/10], Step [211/782], Loss: 2.3011, batch time: 6.728161096572876\n",
      "Epoch [1/10], Step [212/782], Loss: 2.2972, batch time: 6.835567235946655\n",
      "Epoch [1/10], Step [213/782], Loss: 2.3038, batch time: 6.773452281951904\n",
      "Epoch [1/10], Step [214/782], Loss: 2.3039, batch time: 6.836565017700195\n",
      "Epoch [1/10], Step [215/782], Loss: 2.3076, batch time: 6.723004579544067\n",
      "Epoch [1/10], Step [216/782], Loss: 2.3051, batch time: 6.796942949295044\n",
      "Epoch [1/10], Step [217/782], Loss: 2.2986, batch time: 6.753502130508423\n",
      "Epoch [1/10], Step [218/782], Loss: 2.3037, batch time: 6.769302845001221\n",
      "Epoch [1/10], Step [219/782], Loss: 2.3069, batch time: 6.8082756996154785\n",
      "Epoch [1/10], Step [220/782], Loss: 2.3020, batch time: 6.735673427581787\n",
      "Epoch [1/10], Step [221/782], Loss: 2.2990, batch time: 6.883262634277344\n",
      "Epoch [1/10], Step [222/782], Loss: 2.2946, batch time: 6.719677686691284\n",
      "Epoch [1/10], Step [223/782], Loss: 2.2990, batch time: 6.874226808547974\n",
      "Epoch [1/10], Step [224/782], Loss: 2.3062, batch time: 6.726719856262207\n",
      "Epoch [1/10], Step [225/782], Loss: 2.2930, batch time: 6.790263652801514\n",
      "Epoch [1/10], Step [226/782], Loss: 2.3035, batch time: 6.732635736465454\n",
      "Epoch [1/10], Step [227/782], Loss: 2.2990, batch time: 6.792568206787109\n",
      "Epoch [1/10], Step [228/782], Loss: 2.3060, batch time: 6.868000030517578\n",
      "Epoch [1/10], Step [229/782], Loss: 2.3093, batch time: 6.80440092086792\n",
      "Epoch [1/10], Step [230/782], Loss: 2.3107, batch time: 6.797999858856201\n",
      "Epoch [1/10], Step [231/782], Loss: 2.3041, batch time: 6.741886138916016\n",
      "Epoch [1/10], Step [232/782], Loss: 2.3052, batch time: 6.782130479812622\n",
      "Epoch [1/10], Step [233/782], Loss: 2.3126, batch time: 6.7922070026397705\n",
      "Epoch [1/10], Step [234/782], Loss: 2.3021, batch time: 6.808260679244995\n",
      "Epoch [1/10], Step [235/782], Loss: 2.3027, batch time: 6.744901180267334\n",
      "Epoch [1/10], Step [236/782], Loss: 2.3016, batch time: 6.687246084213257\n",
      "Epoch [1/10], Step [237/782], Loss: 2.3028, batch time: 6.769448518753052\n",
      "Epoch [1/10], Step [238/782], Loss: 2.3014, batch time: 6.69065260887146\n",
      "Epoch [1/10], Step [239/782], Loss: 2.2965, batch time: 6.808945655822754\n",
      "Epoch [1/10], Step [240/782], Loss: 2.3008, batch time: 6.717913866043091\n",
      "Epoch [1/10], Step [241/782], Loss: 2.3072, batch time: 6.813747406005859\n",
      "Epoch [1/10], Step [242/782], Loss: 2.3049, batch time: 6.74010443687439\n",
      "Epoch [1/10], Step [243/782], Loss: 2.3063, batch time: 6.797196865081787\n",
      "Epoch [1/10], Step [244/782], Loss: 2.3016, batch time: 6.774638891220093\n",
      "Epoch [1/10], Step [245/782], Loss: 2.3093, batch time: 6.701066493988037\n",
      "Epoch [1/10], Step [246/782], Loss: 2.3059, batch time: 6.784227609634399\n",
      "Epoch [1/10], Step [247/782], Loss: 2.2991, batch time: 6.723387956619263\n",
      "Epoch [1/10], Step [248/782], Loss: 2.3115, batch time: 6.781510353088379\n",
      "Epoch [1/10], Step [249/782], Loss: 2.2907, batch time: 6.725080966949463\n",
      "Epoch [1/10], Step [250/782], Loss: 2.3042, batch time: 6.774626970291138\n",
      "Epoch [1/10], Step [251/782], Loss: 2.3094, batch time: 6.733045339584351\n",
      "Epoch [1/10], Step [252/782], Loss: 2.2970, batch time: 6.770565986633301\n",
      "Epoch [1/10], Step [253/782], Loss: 2.3059, batch time: 6.821759939193726\n",
      "Epoch [1/10], Step [254/782], Loss: 2.3017, batch time: 6.729281663894653\n",
      "Epoch [1/10], Step [255/782], Loss: 2.3036, batch time: 6.791999340057373\n",
      "Epoch [1/10], Step [256/782], Loss: 2.2981, batch time: 6.709044456481934\n",
      "Epoch [1/10], Step [257/782], Loss: 2.3003, batch time: 6.785975456237793\n",
      "Epoch [1/10], Step [258/782], Loss: 2.3093, batch time: 6.708828926086426\n",
      "Epoch [1/10], Step [259/782], Loss: 2.2992, batch time: 6.769381046295166\n",
      "Epoch [1/10], Step [260/782], Loss: 2.2983, batch time: 6.893947124481201\n",
      "Epoch [1/10], Step [261/782], Loss: 2.3033, batch time: 6.74608850479126\n",
      "Epoch [1/10], Step [262/782], Loss: 2.2942, batch time: 6.782251358032227\n",
      "Epoch [1/10], Step [263/782], Loss: 2.3028, batch time: 6.736773729324341\n",
      "Epoch [1/10], Step [264/782], Loss: 2.3104, batch time: 6.832580089569092\n",
      "Epoch [1/10], Step [265/782], Loss: 2.3064, batch time: 6.73384952545166\n",
      "Epoch [1/10], Step [266/782], Loss: 2.2978, batch time: 6.802402973175049\n",
      "Epoch [1/10], Step [267/782], Loss: 2.3021, batch time: 6.707282781600952\n",
      "Epoch [1/10], Step [268/782], Loss: 2.3030, batch time: 6.806297063827515\n",
      "Epoch [1/10], Step [269/782], Loss: 2.3000, batch time: 6.800570487976074\n",
      "Epoch [1/10], Step [270/782], Loss: 2.3015, batch time: 6.652690887451172\n",
      "Epoch [1/10], Step [271/782], Loss: 2.3068, batch time: 6.714495897293091\n",
      "Epoch [1/10], Step [272/782], Loss: 2.3041, batch time: 6.64993691444397\n",
      "Epoch [1/10], Step [273/782], Loss: 2.3066, batch time: 6.742950677871704\n",
      "Epoch [1/10], Step [274/782], Loss: 2.2963, batch time: 6.6489479541778564\n",
      "Epoch [1/10], Step [275/782], Loss: 2.3036, batch time: 6.7273030281066895\n",
      "Epoch [1/10], Step [276/782], Loss: 2.3133, batch time: 6.882526636123657\n",
      "Epoch [1/10], Step [277/782], Loss: 2.3009, batch time: 6.712659120559692\n",
      "Epoch [1/10], Step [278/782], Loss: 2.3099, batch time: 6.791317701339722\n",
      "Epoch [1/10], Step [279/782], Loss: 2.2987, batch time: 6.682430267333984\n",
      "Epoch [1/10], Step [280/782], Loss: 2.3008, batch time: 6.725848913192749\n",
      "Epoch [1/10], Step [281/782], Loss: 2.3031, batch time: 6.663975954055786\n",
      "Epoch [1/10], Step [282/782], Loss: 2.3066, batch time: 6.730402231216431\n",
      "Epoch [1/10], Step [283/782], Loss: 2.2935, batch time: 6.649775743484497\n",
      "Epoch [1/10], Step [284/782], Loss: 2.3032, batch time: 6.726699113845825\n",
      "Epoch [1/10], Step [285/782], Loss: 2.3048, batch time: 6.737789630889893\n",
      "Epoch [1/10], Step [286/782], Loss: 2.2957, batch time: 6.715103626251221\n",
      "Epoch [1/10], Step [287/782], Loss: 2.3121, batch time: 6.718780279159546\n",
      "Epoch [1/10], Step [288/782], Loss: 2.3090, batch time: 6.6514198780059814\n",
      "Epoch [1/10], Step [289/782], Loss: 2.3026, batch time: 6.731997489929199\n",
      "Epoch [1/10], Step [290/782], Loss: 2.3062, batch time: 6.666309356689453\n",
      "Epoch [1/10], Step [291/782], Loss: 2.3026, batch time: 6.709328651428223\n",
      "Epoch [1/10], Step [292/782], Loss: 2.2937, batch time: 6.652101755142212\n",
      "Epoch [1/10], Step [293/782], Loss: 2.2962, batch time: 6.75850772857666\n",
      "Epoch [1/10], Step [294/782], Loss: 2.3113, batch time: 6.749093770980835\n",
      "Epoch [1/10], Step [295/782], Loss: 2.3062, batch time: 6.657593250274658\n",
      "Epoch [1/10], Step [296/782], Loss: 2.3019, batch time: 6.751226902008057\n",
      "Epoch [1/10], Step [297/782], Loss: 2.3068, batch time: 6.666978120803833\n",
      "Epoch [1/10], Step [298/782], Loss: 2.3020, batch time: 6.799753665924072\n",
      "Epoch [1/10], Step [299/782], Loss: 2.3047, batch time: 6.674484729766846\n",
      "Epoch [1/10], Step [300/782], Loss: 2.3027, batch time: 6.770018815994263\n",
      "Epoch [1/10], Step [301/782], Loss: 2.3035, batch time: 6.677375793457031\n",
      "Epoch [1/10], Step [302/782], Loss: 2.2981, batch time: 6.749910593032837\n",
      "Epoch [1/10], Step [303/782], Loss: 2.2962, batch time: 6.738487482070923\n",
      "Epoch [1/10], Step [304/782], Loss: 2.2981, batch time: 6.663767099380493\n",
      "Epoch [1/10], Step [305/782], Loss: 2.3045, batch time: 6.749036073684692\n",
      "Epoch [1/10], Step [306/782], Loss: 2.3019, batch time: 6.655686855316162\n",
      "Epoch [1/10], Step [307/782], Loss: 2.3069, batch time: 6.758527517318726\n",
      "Epoch [1/10], Step [308/782], Loss: 2.3003, batch time: 6.661708831787109\n",
      "Epoch [1/10], Step [309/782], Loss: 2.3045, batch time: 6.719020128250122\n",
      "Epoch [1/10], Step [310/782], Loss: 2.2992, batch time: 6.721691608428955\n",
      "Epoch [1/10], Step [311/782], Loss: 2.3034, batch time: 6.735523700714111\n",
      "Epoch [1/10], Step [312/782], Loss: 2.3050, batch time: 6.737912178039551\n",
      "Epoch [1/10], Step [313/782], Loss: 2.3010, batch time: 6.676670551300049\n",
      "Epoch [1/10], Step [314/782], Loss: 2.3064, batch time: 6.816213369369507\n",
      "Epoch [1/10], Step [315/782], Loss: 2.3072, batch time: 6.669013261795044\n",
      "Epoch [1/10], Step [316/782], Loss: 2.3068, batch time: 6.886960506439209\n",
      "Epoch [1/10], Step [317/782], Loss: 2.3021, batch time: 6.658747673034668\n",
      "Epoch [1/10], Step [318/782], Loss: 2.3025, batch time: 6.752135276794434\n",
      "Epoch [1/10], Step [319/782], Loss: 2.3021, batch time: 6.779693603515625\n",
      "Epoch [1/10], Step [320/782], Loss: 2.2998, batch time: 6.7994537353515625\n",
      "Epoch [1/10], Step [321/782], Loss: 2.3036, batch time: 6.738920211791992\n",
      "Epoch [1/10], Step [322/782], Loss: 2.3086, batch time: 6.709662675857544\n",
      "Epoch [1/10], Step [323/782], Loss: 2.3083, batch time: 6.762616395950317\n",
      "Epoch [1/10], Step [324/782], Loss: 2.3089, batch time: 6.688211917877197\n",
      "Epoch [1/10], Step [325/782], Loss: 2.3074, batch time: 6.752520322799683\n",
      "Epoch [1/10], Step [326/782], Loss: 2.3049, batch time: 6.700831413269043\n",
      "Epoch [1/10], Step [327/782], Loss: 2.3077, batch time: 6.762624025344849\n",
      "Epoch [1/10], Step [328/782], Loss: 2.3087, batch time: 6.733075857162476\n",
      "Epoch [1/10], Step [329/782], Loss: 2.3041, batch time: 6.662179231643677\n",
      "Epoch [1/10], Step [330/782], Loss: 2.3003, batch time: 6.726975202560425\n",
      "Epoch [1/10], Step [331/782], Loss: 2.3037, batch time: 6.693053245544434\n",
      "Epoch [1/10], Step [332/782], Loss: 2.2897, batch time: 6.72092604637146\n",
      "Epoch [1/10], Step [333/782], Loss: 2.3059, batch time: 6.671858072280884\n",
      "Epoch [1/10], Step [334/782], Loss: 2.3057, batch time: 6.74379825592041\n",
      "Epoch [1/10], Step [335/782], Loss: 2.3040, batch time: 6.733709812164307\n",
      "Epoch [1/10], Step [336/782], Loss: 2.3045, batch time: 6.656454801559448\n",
      "Epoch [1/10], Step [337/782], Loss: 2.3031, batch time: 6.754144906997681\n",
      "Epoch [1/10], Step [338/782], Loss: 2.3109, batch time: 6.720663547515869\n",
      "Epoch [1/10], Step [339/782], Loss: 2.2997, batch time: 6.7711403369903564\n",
      "Epoch [1/10], Step [340/782], Loss: 2.3013, batch time: 6.688375949859619\n",
      "Epoch [1/10], Step [341/782], Loss: 2.2988, batch time: 6.7479329109191895\n",
      "Epoch [1/10], Step [342/782], Loss: 2.3067, batch time: 6.6803297996521\n",
      "Epoch [1/10], Step [343/782], Loss: 2.3019, batch time: 6.79207444190979\n",
      "Epoch [1/10], Step [344/782], Loss: 2.3072, batch time: 6.746538162231445\n",
      "Epoch [1/10], Step [345/782], Loss: 2.3159, batch time: 6.799564599990845\n",
      "Epoch [1/10], Step [346/782], Loss: 2.3032, batch time: 6.7203898429870605\n",
      "Epoch [1/10], Step [347/782], Loss: 2.2920, batch time: 6.6604673862457275\n",
      "Epoch [1/10], Step [348/782], Loss: 2.3012, batch time: 6.752272129058838\n",
      "Epoch [1/10], Step [349/782], Loss: 2.2995, batch time: 6.689320087432861\n",
      "Epoch [1/10], Step [350/782], Loss: 2.3058, batch time: 6.769599437713623\n",
      "Epoch [1/10], Step [351/782], Loss: 2.2908, batch time: 6.678936719894409\n",
      "Epoch [1/10], Step [352/782], Loss: 2.2975, batch time: 6.735089302062988\n",
      "Epoch [1/10], Step [353/782], Loss: 2.2988, batch time: 6.79373836517334\n",
      "Epoch [1/10], Step [354/782], Loss: 2.3011, batch time: 6.68915581703186\n",
      "Epoch [1/10], Step [355/782], Loss: 2.2970, batch time: 6.737989187240601\n",
      "Epoch [1/10], Step [356/782], Loss: 2.3000, batch time: 6.722932577133179\n",
      "Epoch [1/10], Step [357/782], Loss: 2.3083, batch time: 6.840927600860596\n",
      "Epoch [1/10], Step [358/782], Loss: 2.3064, batch time: 6.669074058532715\n",
      "Epoch [1/10], Step [359/782], Loss: 2.3022, batch time: 6.748893976211548\n",
      "Epoch [1/10], Step [360/782], Loss: 2.3019, batch time: 6.739886999130249\n",
      "Epoch [1/10], Step [361/782], Loss: 2.3093, batch time: 6.6876819133758545\n",
      "Epoch [1/10], Step [362/782], Loss: 2.2985, batch time: 6.7581446170806885\n",
      "Epoch [1/10], Step [363/782], Loss: 2.3018, batch time: 6.720921039581299\n",
      "Epoch [1/10], Step [364/782], Loss: 2.2951, batch time: 6.7314043045043945\n",
      "Epoch [1/10], Step [365/782], Loss: 2.2990, batch time: 6.688177585601807\n",
      "Epoch [1/10], Step [366/782], Loss: 2.3024, batch time: 6.741495132446289\n",
      "Epoch [1/10], Step [367/782], Loss: 2.3009, batch time: 6.662913084030151\n",
      "Epoch [1/10], Step [368/782], Loss: 2.2988, batch time: 6.731435060501099\n",
      "Epoch [1/10], Step [369/782], Loss: 2.3028, batch time: 6.790172338485718\n",
      "Epoch [1/10], Step [370/782], Loss: 2.2920, batch time: 6.674429178237915\n",
      "Epoch [1/10], Step [371/782], Loss: 2.2945, batch time: 6.739268064498901\n",
      "Epoch [1/10], Step [372/782], Loss: 2.3055, batch time: 6.649150848388672\n",
      "Epoch [1/10], Step [373/782], Loss: 2.3010, batch time: 6.730554103851318\n",
      "Epoch [1/10], Step [374/782], Loss: 2.3004, batch time: 6.6760780811309814\n",
      "Epoch [1/10], Step [375/782], Loss: 2.3097, batch time: 6.726850509643555\n",
      "Epoch [1/10], Step [376/782], Loss: 2.3057, batch time: 6.655166149139404\n",
      "Epoch [1/10], Step [377/782], Loss: 2.3017, batch time: 6.733235597610474\n",
      "Epoch [1/10], Step [378/782], Loss: 2.3075, batch time: 6.727368116378784\n",
      "Epoch [1/10], Step [379/782], Loss: 2.3022, batch time: 6.666248559951782\n",
      "Epoch [1/10], Step [380/782], Loss: 2.3053, batch time: 6.736774682998657\n",
      "Epoch [1/10], Step [381/782], Loss: 2.3001, batch time: 6.663628101348877\n",
      "Epoch [1/10], Step [382/782], Loss: 2.3106, batch time: 6.730848789215088\n",
      "Epoch [1/10], Step [383/782], Loss: 2.3062, batch time: 6.667778968811035\n",
      "Epoch [1/10], Step [384/782], Loss: 2.3091, batch time: 6.725356340408325\n",
      "Epoch [1/10], Step [385/782], Loss: 2.3007, batch time: 6.728143930435181\n",
      "Epoch [1/10], Step [386/782], Loss: 2.3043, batch time: 6.652862548828125\n",
      "Epoch [1/10], Step [387/782], Loss: 2.3085, batch time: 6.727076530456543\n",
      "Epoch [1/10], Step [388/782], Loss: 2.2940, batch time: 6.651786804199219\n",
      "Epoch [1/10], Step [389/782], Loss: 2.3051, batch time: 6.746376037597656\n",
      "Epoch [1/10], Step [390/782], Loss: 2.3005, batch time: 6.754673719406128\n",
      "Epoch [1/10], Step [391/782], Loss: 2.3009, batch time: 6.735215902328491\n",
      "Epoch [1/10], Step [392/782], Loss: 2.3064, batch time: 6.723255395889282\n",
      "Epoch [1/10], Step [393/782], Loss: 2.2981, batch time: 6.734111309051514\n",
      "Epoch [1/10], Step [394/782], Loss: 2.3060, batch time: 6.7306129932403564\n",
      "Epoch [1/10], Step [395/782], Loss: 2.3035, batch time: 6.65925145149231\n",
      "Epoch [1/10], Step [396/782], Loss: 2.3074, batch time: 6.766090393066406\n",
      "Epoch [1/10], Step [397/782], Loss: 2.3008, batch time: 6.656303882598877\n",
      "Epoch [1/10], Step [398/782], Loss: 2.3014, batch time: 6.7429304122924805\n",
      "Epoch [1/10], Step [399/782], Loss: 2.3050, batch time: 6.701305627822876\n",
      "Epoch [1/10], Step [400/782], Loss: 2.3033, batch time: 6.733165740966797\n",
      "Epoch [1/10], Step [401/782], Loss: 2.3029, batch time: 6.673825740814209\n",
      "Epoch [1/10], Step [402/782], Loss: 2.3074, batch time: 6.803947687149048\n",
      "Epoch [1/10], Step [403/782], Loss: 2.3079, batch time: 6.734000205993652\n",
      "Epoch [1/10], Step [404/782], Loss: 2.3032, batch time: 6.685068607330322\n",
      "Epoch [1/10], Step [405/782], Loss: 2.3093, batch time: 6.740728855133057\n",
      "Epoch [1/10], Step [406/782], Loss: 2.3018, batch time: 6.658751726150513\n",
      "Epoch [1/10], Step [407/782], Loss: 2.2986, batch time: 6.744099140167236\n",
      "Epoch [1/10], Step [408/782], Loss: 2.3028, batch time: 6.658714771270752\n",
      "Epoch [1/10], Step [409/782], Loss: 2.2990, batch time: 6.721113443374634\n",
      "Epoch [1/10], Step [410/782], Loss: 2.2986, batch time: 6.752578020095825\n",
      "Epoch [1/10], Step [411/782], Loss: 2.3032, batch time: 6.674273490905762\n",
      "Epoch [1/10], Step [412/782], Loss: 2.3125, batch time: 6.761382579803467\n",
      "Epoch [1/10], Step [413/782], Loss: 2.3040, batch time: 6.669421195983887\n",
      "Epoch [1/10], Step [414/782], Loss: 2.3152, batch time: 6.72392463684082\n",
      "Epoch [1/10], Step [415/782], Loss: 2.2998, batch time: 6.761913537979126\n",
      "Epoch [1/10], Step [416/782], Loss: 2.3116, batch time: 6.7196855545043945\n",
      "Epoch [1/10], Step [417/782], Loss: 2.3137, batch time: 6.693985223770142\n",
      "Epoch [1/10], Step [418/782], Loss: 2.3103, batch time: 6.728758811950684\n",
      "Epoch [1/10], Step [419/782], Loss: 2.3098, batch time: 6.783047914505005\n",
      "Epoch [1/10], Step [420/782], Loss: 2.3081, batch time: 6.694953918457031\n",
      "Epoch [1/10], Step [421/782], Loss: 2.3041, batch time: 6.733489274978638\n",
      "Epoch [1/10], Step [422/782], Loss: 2.3121, batch time: 6.66865086555481\n",
      "Epoch [1/10], Step [423/782], Loss: 2.3150, batch time: 6.736741304397583\n",
      "Epoch [1/10], Step [424/782], Loss: 2.3033, batch time: 6.6569507122039795\n",
      "Epoch [1/10], Step [425/782], Loss: 2.3007, batch time: 6.725794076919556\n",
      "Epoch [1/10], Step [426/782], Loss: 2.2967, batch time: 6.683842182159424\n",
      "Epoch [1/10], Step [427/782], Loss: 2.3018, batch time: 6.728421449661255\n",
      "Epoch [1/10], Step [428/782], Loss: 2.2999, batch time: 6.719216823577881\n",
      "Epoch [1/10], Step [429/782], Loss: 2.3036, batch time: 6.671686887741089\n",
      "Epoch [1/10], Step [430/782], Loss: 2.3047, batch time: 6.7124176025390625\n",
      "Epoch [1/10], Step [431/782], Loss: 2.3059, batch time: 6.645216941833496\n",
      "Epoch [1/10], Step [432/782], Loss: 2.2994, batch time: 6.719158172607422\n",
      "Epoch [1/10], Step [433/782], Loss: 2.2944, batch time: 6.65979528427124\n",
      "Epoch [1/10], Step [434/782], Loss: 2.3031, batch time: 6.709874629974365\n",
      "Epoch [1/10], Step [435/782], Loss: 2.3092, batch time: 6.71493124961853\n",
      "Epoch [1/10], Step [436/782], Loss: 2.3102, batch time: 6.65733003616333\n",
      "Epoch [1/10], Step [437/782], Loss: 2.3010, batch time: 6.726275682449341\n",
      "Epoch [1/10], Step [438/782], Loss: 2.2983, batch time: 6.6551878452301025\n",
      "Epoch [1/10], Step [439/782], Loss: 2.3023, batch time: 6.719178676605225\n",
      "Epoch [1/10], Step [440/782], Loss: 2.3018, batch time: 6.663315296173096\n",
      "Epoch [1/10], Step [441/782], Loss: 2.3010, batch time: 6.723154067993164\n",
      "Epoch [1/10], Step [442/782], Loss: 2.3026, batch time: 6.646301031112671\n",
      "Epoch [1/10], Step [443/782], Loss: 2.3012, batch time: 6.731105327606201\n",
      "Epoch [1/10], Step [444/782], Loss: 2.3039, batch time: 6.7172980308532715\n",
      "Epoch [1/10], Step [445/782], Loss: 2.3049, batch time: 6.656496524810791\n",
      "Epoch [1/10], Step [446/782], Loss: 2.3010, batch time: 6.7856528759002686\n",
      "Epoch [1/10], Step [447/782], Loss: 2.2965, batch time: 6.967657566070557\n",
      "Epoch [1/10], Step [448/782], Loss: 2.3019, batch time: 6.890512466430664\n",
      "Epoch [1/10], Step [449/782], Loss: 2.3020, batch time: 6.905226945877075\n",
      "Epoch [1/10], Step [450/782], Loss: 2.2995, batch time: 7.086935043334961\n",
      "Epoch [1/10], Step [451/782], Loss: 2.3040, batch time: 7.047242641448975\n",
      "Epoch [1/10], Step [452/782], Loss: 2.3001, batch time: 6.995251655578613\n",
      "Epoch [1/10], Step [453/782], Loss: 2.3006, batch time: 7.161067247390747\n",
      "Epoch [1/10], Step [454/782], Loss: 2.2950, batch time: 6.97235369682312\n",
      "Epoch [1/10], Step [455/782], Loss: 2.3044, batch time: 7.06052827835083\n",
      "Epoch [1/10], Step [456/782], Loss: 2.3030, batch time: 6.872339725494385\n",
      "Epoch [1/10], Step [457/782], Loss: 2.3078, batch time: 7.028737306594849\n",
      "Epoch [1/10], Step [458/782], Loss: 2.3151, batch time: 6.843804359436035\n",
      "Epoch [1/10], Step [459/782], Loss: 2.2996, batch time: 7.041812181472778\n",
      "Epoch [1/10], Step [460/782], Loss: 2.3037, batch time: 6.913007736206055\n",
      "Epoch [1/10], Step [461/782], Loss: 2.3043, batch time: 6.942828178405762\n",
      "Epoch [1/10], Step [462/782], Loss: 2.3065, batch time: 6.9084672927856445\n",
      "Epoch [1/10], Step [463/782], Loss: 2.3054, batch time: 6.928936243057251\n",
      "Epoch [1/10], Step [464/782], Loss: 2.3109, batch time: 7.009090900421143\n",
      "Epoch [1/10], Step [465/782], Loss: 2.3057, batch time: 7.152598142623901\n",
      "Epoch [1/10], Step [466/782], Loss: 2.3030, batch time: 7.058265686035156\n",
      "Epoch [1/10], Step [467/782], Loss: 2.3076, batch time: 6.999938249588013\n",
      "Epoch [1/10], Step [468/782], Loss: 2.3011, batch time: 7.012562274932861\n",
      "Epoch [1/10], Step [469/782], Loss: 2.3044, batch time: 6.974053382873535\n",
      "Epoch [1/10], Step [470/782], Loss: 2.3002, batch time: 6.9621899127960205\n",
      "Epoch [1/10], Step [471/782], Loss: 2.3109, batch time: 7.184422254562378\n",
      "Epoch [1/10], Step [472/782], Loss: 2.3056, batch time: 6.92662787437439\n",
      "Epoch [1/10], Step [473/782], Loss: 2.3067, batch time: 7.08635687828064\n",
      "Epoch [1/10], Step [474/782], Loss: 2.3007, batch time: 7.035816431045532\n",
      "Epoch [1/10], Step [475/782], Loss: 2.3013, batch time: 7.187173843383789\n",
      "Epoch [1/10], Step [476/782], Loss: 2.3058, batch time: 6.888814210891724\n",
      "Epoch [1/10], Step [477/782], Loss: 2.3050, batch time: 7.16010856628418\n",
      "Epoch [1/10], Step [478/782], Loss: 2.3032, batch time: 6.984188795089722\n",
      "Epoch [1/10], Step [479/782], Loss: 2.3039, batch time: 6.906095027923584\n",
      "Epoch [1/10], Step [480/782], Loss: 2.3087, batch time: 7.175495624542236\n",
      "Epoch [1/10], Step [481/782], Loss: 2.2983, batch time: 7.05600118637085\n",
      "Epoch [1/10], Step [482/782], Loss: 2.3017, batch time: 7.012283802032471\n",
      "Epoch [1/10], Step [483/782], Loss: 2.2976, batch time: 6.900328159332275\n",
      "Epoch [1/10], Step [484/782], Loss: 2.2957, batch time: 7.036341905593872\n",
      "Epoch [1/10], Step [485/782], Loss: 2.2924, batch time: 7.134031772613525\n",
      "Epoch [1/10], Step [486/782], Loss: 2.2999, batch time: 7.102360963821411\n",
      "Epoch [1/10], Step [487/782], Loss: 2.3045, batch time: 7.124689340591431\n",
      "Epoch [1/10], Step [488/782], Loss: 2.3080, batch time: 7.079575777053833\n",
      "Epoch [1/10], Step [489/782], Loss: 2.3028, batch time: 7.041727542877197\n",
      "Epoch [1/10], Step [490/782], Loss: 2.3006, batch time: 7.059846639633179\n",
      "Epoch [1/10], Step [491/782], Loss: 2.2979, batch time: 7.050639390945435\n",
      "Epoch [1/10], Step [492/782], Loss: 2.2977, batch time: 7.074883937835693\n",
      "Epoch [1/10], Step [493/782], Loss: 2.3002, batch time: 7.084187746047974\n",
      "Epoch [1/10], Step [494/782], Loss: 2.2981, batch time: 7.141269683837891\n",
      "Epoch [1/10], Step [495/782], Loss: 2.3095, batch time: 6.903721332550049\n",
      "Epoch [1/10], Step [496/782], Loss: 2.2966, batch time: 6.994447946548462\n",
      "Epoch [1/10], Step [497/782], Loss: 2.3086, batch time: 6.965968608856201\n",
      "Epoch [1/10], Step [498/782], Loss: 2.3037, batch time: 7.138857126235962\n",
      "Epoch [1/10], Step [499/782], Loss: 2.3079, batch time: 7.023106575012207\n",
      "Epoch [1/10], Step [500/782], Loss: 2.3069, batch time: 7.080185890197754\n",
      "Epoch [1/10], Step [501/782], Loss: 2.3040, batch time: 6.931831359863281\n",
      "Epoch [1/10], Step [502/782], Loss: 2.3048, batch time: 6.927464008331299\n",
      "Epoch [1/10], Step [503/782], Loss: 2.3100, batch time: 7.136870861053467\n",
      "Epoch [1/10], Step [504/782], Loss: 2.3086, batch time: 6.986492395401001\n",
      "Epoch [1/10], Step [505/782], Loss: 2.3072, batch time: 7.1710405349731445\n",
      "Epoch [1/10], Step [506/782], Loss: 2.3132, batch time: 7.116947650909424\n",
      "Epoch [1/10], Step [507/782], Loss: 2.3073, batch time: 6.946708679199219\n",
      "Epoch [1/10], Step [508/782], Loss: 2.2997, batch time: 7.229562520980835\n",
      "Epoch [1/10], Step [509/782], Loss: 2.3081, batch time: 6.979833126068115\n",
      "Epoch [1/10], Step [510/782], Loss: 2.3066, batch time: 7.131372928619385\n",
      "Epoch [1/10], Step [511/782], Loss: 2.3018, batch time: 7.02035665512085\n",
      "Epoch [1/10], Step [512/782], Loss: 2.3112, batch time: 7.052330493927002\n",
      "Epoch [1/10], Step [513/782], Loss: 2.3050, batch time: 7.1342387199401855\n",
      "Epoch [1/10], Step [514/782], Loss: 2.3123, batch time: 7.12104606628418\n",
      "Epoch [1/10], Step [515/782], Loss: 2.3124, batch time: 7.183820724487305\n",
      "Epoch [1/10], Step [516/782], Loss: 2.3060, batch time: 6.9475390911102295\n",
      "Epoch [1/10], Step [517/782], Loss: 2.3095, batch time: 6.985534906387329\n",
      "Epoch [1/10], Step [518/782], Loss: 2.3077, batch time: 7.003819704055786\n",
      "Epoch [1/10], Step [519/782], Loss: 2.3041, batch time: 7.1088292598724365\n",
      "Epoch [1/10], Step [520/782], Loss: 2.2950, batch time: 7.094990015029907\n",
      "Epoch [1/10], Step [521/782], Loss: 2.3013, batch time: 6.952277898788452\n",
      "Epoch [1/10], Step [522/782], Loss: 2.3054, batch time: 7.031496047973633\n",
      "Epoch [1/10], Step [523/782], Loss: 2.2984, batch time: 7.226918458938599\n",
      "Epoch [1/10], Step [524/782], Loss: 2.3033, batch time: 6.894235610961914\n",
      "Epoch [1/10], Step [525/782], Loss: 2.3076, batch time: 6.9357664585113525\n",
      "Epoch [1/10], Step [526/782], Loss: 2.2968, batch time: 7.065454006195068\n",
      "Epoch [1/10], Step [527/782], Loss: 2.3013, batch time: 6.994548559188843\n",
      "Epoch [1/10], Step [528/782], Loss: 2.2991, batch time: 6.995809316635132\n",
      "Epoch [1/10], Step [529/782], Loss: 2.3018, batch time: 7.11525559425354\n",
      "Epoch [1/10], Step [530/782], Loss: 2.3033, batch time: 6.9130940437316895\n",
      "Epoch [1/10], Step [531/782], Loss: 2.3038, batch time: 6.938656568527222\n",
      "Epoch [1/10], Step [532/782], Loss: 2.3078, batch time: 6.799076557159424\n",
      "Epoch [1/10], Step [533/782], Loss: 2.2933, batch time: 6.658606052398682\n",
      "Epoch [1/10], Step [534/782], Loss: 2.3145, batch time: 6.711467027664185\n",
      "Epoch [1/10], Step [535/782], Loss: 2.3064, batch time: 6.7171549797058105\n",
      "Epoch [1/10], Step [536/782], Loss: 2.3040, batch time: 6.753918409347534\n",
      "Epoch [1/10], Step [537/782], Loss: 2.3026, batch time: 6.801145553588867\n",
      "Epoch [1/10], Step [538/782], Loss: 2.3021, batch time: 6.6404290199279785\n",
      "Epoch [1/10], Step [539/782], Loss: 2.2941, batch time: 6.733977317810059\n",
      "Epoch [1/10], Step [540/782], Loss: 2.3100, batch time: 6.681881666183472\n",
      "Epoch [1/10], Step [541/782], Loss: 2.2999, batch time: 6.747786521911621\n",
      "Epoch [1/10], Step [542/782], Loss: 2.2993, batch time: 6.669149875640869\n",
      "Epoch [1/10], Step [543/782], Loss: 2.2973, batch time: 6.713579893112183\n",
      "Epoch [1/10], Step [544/782], Loss: 2.3032, batch time: 6.748609781265259\n",
      "Epoch [1/10], Step [545/782], Loss: 2.3036, batch time: 6.695946931838989\n",
      "Epoch [1/10], Step [546/782], Loss: 2.3092, batch time: 6.724823951721191\n",
      "Epoch [1/10], Step [547/782], Loss: 2.3032, batch time: 6.736773252487183\n",
      "Epoch [1/10], Step [548/782], Loss: 2.3016, batch time: 6.725572824478149\n",
      "Epoch [1/10], Step [549/782], Loss: 2.3040, batch time: 6.735043048858643\n",
      "Epoch [1/10], Step [550/782], Loss: 2.3036, batch time: 6.753759384155273\n",
      "Epoch [1/10], Step [551/782], Loss: 2.3050, batch time: 6.658035039901733\n",
      "Epoch [1/10], Step [552/782], Loss: 2.3011, batch time: 6.718960285186768\n",
      "Epoch [1/10], Step [553/782], Loss: 2.3021, batch time: 6.745365381240845\n",
      "Epoch [1/10], Step [554/782], Loss: 2.2985, batch time: 6.799968242645264\n",
      "Epoch [1/10], Step [555/782], Loss: 2.3063, batch time: 6.729140520095825\n",
      "Epoch [1/10], Step [556/782], Loss: 2.3089, batch time: 6.6441357135772705\n",
      "Epoch [1/10], Step [557/782], Loss: 2.3047, batch time: 6.793520927429199\n",
      "Epoch [1/10], Step [558/782], Loss: 2.2969, batch time: 6.656135559082031\n",
      "Epoch [1/10], Step [559/782], Loss: 2.3016, batch time: 6.740548133850098\n",
      "Epoch [1/10], Step [560/782], Loss: 2.3051, batch time: 6.731459140777588\n",
      "Epoch [1/10], Step [561/782], Loss: 2.2990, batch time: 6.647563695907593\n",
      "Epoch [1/10], Step [562/782], Loss: 2.2928, batch time: 6.746415615081787\n",
      "Epoch [1/10], Step [563/782], Loss: 2.3054, batch time: 6.655678987503052\n",
      "Epoch [1/10], Step [564/782], Loss: 2.3100, batch time: 6.7423481941223145\n",
      "Epoch [1/10], Step [565/782], Loss: 2.3062, batch time: 6.6997480392456055\n",
      "Epoch [1/10], Step [566/782], Loss: 2.3015, batch time: 6.708250045776367\n",
      "Epoch [1/10], Step [567/782], Loss: 2.3095, batch time: 6.672358512878418\n",
      "Epoch [1/10], Step [568/782], Loss: 2.3071, batch time: 6.737642526626587\n",
      "Epoch [1/10], Step [569/782], Loss: 2.3006, batch time: 6.780118227005005\n",
      "Epoch [1/10], Step [570/782], Loss: 2.3030, batch time: 6.663358926773071\n",
      "Epoch [1/10], Step [571/782], Loss: 2.2958, batch time: 6.751472234725952\n",
      "Epoch [1/10], Step [572/782], Loss: 2.3093, batch time: 6.646238565444946\n",
      "Epoch [1/10], Step [573/782], Loss: 2.3032, batch time: 6.742299795150757\n",
      "Epoch [1/10], Step [574/782], Loss: 2.3007, batch time: 6.640145540237427\n",
      "Epoch [1/10], Step [575/782], Loss: 2.3016, batch time: 6.726829767227173\n",
      "Epoch [1/10], Step [576/782], Loss: 2.3036, batch time: 6.654677867889404\n",
      "Epoch [1/10], Step [577/782], Loss: 2.3024, batch time: 6.723150968551636\n",
      "Epoch [1/10], Step [578/782], Loss: 2.3022, batch time: 6.711001873016357\n",
      "Epoch [1/10], Step [579/782], Loss: 2.3000, batch time: 6.645571708679199\n",
      "Epoch [1/10], Step [580/782], Loss: 2.3021, batch time: 6.721905708312988\n",
      "Epoch [1/10], Step [581/782], Loss: 2.2997, batch time: 6.645179033279419\n",
      "Epoch [1/10], Step [582/782], Loss: 2.3000, batch time: 6.711439371109009\n",
      "Epoch [1/10], Step [583/782], Loss: 2.3037, batch time: 6.652821063995361\n",
      "Epoch [1/10], Step [584/782], Loss: 2.3126, batch time: 6.736938238143921\n",
      "Epoch [1/10], Step [585/782], Loss: 2.3004, batch time: 6.726795434951782\n",
      "Epoch [1/10], Step [586/782], Loss: 2.3057, batch time: 6.655122995376587\n",
      "Epoch [1/10], Step [587/782], Loss: 2.3015, batch time: 6.7632856369018555\n",
      "Epoch [1/10], Step [588/782], Loss: 2.3034, batch time: 6.658735513687134\n",
      "Epoch [1/10], Step [589/782], Loss: 2.2981, batch time: 6.747003555297852\n",
      "Epoch [1/10], Step [590/782], Loss: 2.3003, batch time: 6.655298709869385\n",
      "Epoch [1/10], Step [591/782], Loss: 2.2990, batch time: 6.742427110671997\n",
      "Epoch [1/10], Step [592/782], Loss: 2.3016, batch time: 6.653011322021484\n",
      "Epoch [1/10], Step [593/782], Loss: 2.2999, batch time: 6.74258017539978\n",
      "Epoch [1/10], Step [594/782], Loss: 2.3017, batch time: 6.729975938796997\n",
      "Epoch [1/10], Step [595/782], Loss: 2.3052, batch time: 6.723357200622559\n",
      "Epoch [1/10], Step [596/782], Loss: 2.2997, batch time: 6.739011526107788\n",
      "Epoch [1/10], Step [597/782], Loss: 2.3010, batch time: 6.681062936782837\n",
      "Epoch [1/10], Step [598/782], Loss: 2.2988, batch time: 6.732497692108154\n",
      "Epoch [1/10], Step [599/782], Loss: 2.3055, batch time: 6.699726104736328\n",
      "Epoch [1/10], Step [600/782], Loss: 2.2991, batch time: 6.782078504562378\n",
      "Epoch [1/10], Step [601/782], Loss: 2.3102, batch time: 6.655187129974365\n",
      "Epoch [1/10], Step [602/782], Loss: 2.3093, batch time: 6.735072612762451\n",
      "Epoch [1/10], Step [603/782], Loss: 2.2991, batch time: 6.723661661148071\n",
      "Epoch [1/10], Step [604/782], Loss: 2.3038, batch time: 6.652017116546631\n",
      "Epoch [1/10], Step [605/782], Loss: 2.3015, batch time: 6.737704277038574\n",
      "Epoch [1/10], Step [606/782], Loss: 2.3001, batch time: 6.653424024581909\n",
      "Epoch [1/10], Step [607/782], Loss: 2.3045, batch time: 6.961376428604126\n",
      "Epoch [1/10], Step [608/782], Loss: 2.3026, batch time: 6.651795148849487\n",
      "Epoch [1/10], Step [609/782], Loss: 2.2937, batch time: 6.721179723739624\n",
      "Epoch [1/10], Step [610/782], Loss: 2.3057, batch time: 6.706795930862427\n",
      "Epoch [1/10], Step [611/782], Loss: 2.3015, batch time: 6.661157846450806\n",
      "Epoch [1/10], Step [612/782], Loss: 2.3052, batch time: 6.76941442489624\n",
      "Epoch [1/10], Step [613/782], Loss: 2.3057, batch time: 6.67770528793335\n",
      "Epoch [1/10], Step [614/782], Loss: 2.3016, batch time: 6.714853763580322\n",
      "Epoch [1/10], Step [615/782], Loss: 2.3115, batch time: 6.745447158813477\n",
      "Epoch [1/10], Step [616/782], Loss: 2.3034, batch time: 6.720942974090576\n",
      "Epoch [1/10], Step [617/782], Loss: 2.3007, batch time: 6.644489526748657\n",
      "Epoch [1/10], Step [618/782], Loss: 2.3062, batch time: 6.826858997344971\n",
      "Epoch [1/10], Step [619/782], Loss: 2.3036, batch time: 7.013477325439453\n",
      "Epoch [1/10], Step [620/782], Loss: 2.3042, batch time: 6.752678155899048\n",
      "Epoch [1/10], Step [621/782], Loss: 2.3055, batch time: 6.865911483764648\n",
      "Epoch [1/10], Step [622/782], Loss: 2.3060, batch time: 6.73317551612854\n",
      "Epoch [1/10], Step [623/782], Loss: 2.3120, batch time: 6.811763286590576\n",
      "Epoch [1/10], Step [624/782], Loss: 2.3005, batch time: 6.738137245178223\n",
      "Epoch [1/10], Step [625/782], Loss: 2.3114, batch time: 6.7752087116241455\n",
      "Epoch [1/10], Step [626/782], Loss: 2.3026, batch time: 6.748063087463379\n",
      "Epoch [1/10], Step [627/782], Loss: 2.3087, batch time: 6.770005226135254\n",
      "Epoch [1/10], Step [628/782], Loss: 2.3060, batch time: 6.8499109745025635\n",
      "Epoch [1/10], Step [629/782], Loss: 2.2951, batch time: 6.767256021499634\n",
      "Epoch [1/10], Step [630/782], Loss: 2.3028, batch time: 6.794173955917358\n",
      "Epoch [1/10], Step [631/782], Loss: 2.3026, batch time: 6.732576608657837\n",
      "Epoch [1/10], Step [632/782], Loss: 2.3035, batch time: 6.772907018661499\n",
      "Epoch [1/10], Step [633/782], Loss: 2.3098, batch time: 6.684941291809082\n",
      "Epoch [1/10], Step [634/782], Loss: 2.2975, batch time: 6.779734134674072\n",
      "Epoch [1/10], Step [635/782], Loss: 2.3018, batch time: 6.7875285148620605\n",
      "Epoch [1/10], Step [636/782], Loss: 2.2965, batch time: 6.708052158355713\n",
      "Epoch [1/10], Step [637/782], Loss: 2.3053, batch time: 6.818108081817627\n",
      "Epoch [1/10], Step [638/782], Loss: 2.3012, batch time: 6.784690618515015\n",
      "Epoch [1/10], Step [639/782], Loss: 2.3074, batch time: 6.811192989349365\n",
      "Epoch [1/10], Step [640/782], Loss: 2.3034, batch time: 6.742522239685059\n",
      "Epoch [1/10], Step [641/782], Loss: 2.2877, batch time: 6.799537658691406\n",
      "Epoch [1/10], Step [642/782], Loss: 2.3022, batch time: 6.721951246261597\n",
      "Epoch [1/10], Step [643/782], Loss: 2.3041, batch time: 6.767452716827393\n",
      "Epoch [1/10], Step [644/782], Loss: 2.3053, batch time: 6.7874977588653564\n",
      "Epoch [1/10], Step [645/782], Loss: 2.3076, batch time: 6.704838514328003\n",
      "Epoch [1/10], Step [646/782], Loss: 2.3099, batch time: 6.803165674209595\n",
      "Epoch [1/10], Step [647/782], Loss: 2.2993, batch time: 6.7388811111450195\n",
      "Epoch [1/10], Step [648/782], Loss: 2.3067, batch time: 6.810919523239136\n",
      "Epoch [1/10], Step [649/782], Loss: 2.3087, batch time: 6.715663433074951\n",
      "Epoch [1/10], Step [650/782], Loss: 2.3032, batch time: 6.757088899612427\n",
      "Epoch [1/10], Step [651/782], Loss: 2.3005, batch time: 6.713296175003052\n",
      "Epoch [1/10], Step [652/782], Loss: 2.2944, batch time: 6.804739713668823\n",
      "Epoch [1/10], Step [653/782], Loss: 2.2996, batch time: 6.770956516265869\n",
      "Epoch [1/10], Step [654/782], Loss: 2.3031, batch time: 6.7221999168396\n",
      "Epoch [1/10], Step [655/782], Loss: 2.3044, batch time: 6.801709413528442\n",
      "Epoch [1/10], Step [656/782], Loss: 2.3074, batch time: 6.732188940048218\n",
      "Epoch [1/10], Step [657/782], Loss: 2.2927, batch time: 6.816820383071899\n",
      "Epoch [1/10], Step [658/782], Loss: 2.3077, batch time: 6.711321115493774\n",
      "Epoch [1/10], Step [659/782], Loss: 2.2972, batch time: 6.785563230514526\n",
      "Epoch [1/10], Step [660/782], Loss: 2.3142, batch time: 6.8190765380859375\n",
      "Epoch [1/10], Step [661/782], Loss: 2.3060, batch time: 6.716068744659424\n",
      "Epoch [1/10], Step [662/782], Loss: 2.3104, batch time: 6.7600014209747314\n",
      "Epoch [1/10], Step [663/782], Loss: 2.3118, batch time: 6.714000463485718\n",
      "Epoch [1/10], Step [664/782], Loss: 2.3073, batch time: 6.84325909614563\n",
      "Epoch [1/10], Step [665/782], Loss: 2.3073, batch time: 6.716349124908447\n",
      "Epoch [1/10], Step [666/782], Loss: 2.3058, batch time: 6.810620069503784\n",
      "Epoch [1/10], Step [667/782], Loss: 2.3003, batch time: 6.791727781295776\n",
      "Epoch [1/10], Step [668/782], Loss: 2.3053, batch time: 6.84119725227356\n",
      "Epoch [1/10], Step [669/782], Loss: 2.2957, batch time: 6.7945427894592285\n",
      "Epoch [1/10], Step [670/782], Loss: 2.3032, batch time: 6.6728715896606445\n",
      "Epoch [1/10], Step [671/782], Loss: 2.3007, batch time: 6.764681339263916\n",
      "Epoch [1/10], Step [672/782], Loss: 2.2967, batch time: 6.7266223430633545\n",
      "Epoch [1/10], Step [673/782], Loss: 2.2996, batch time: 6.791426181793213\n",
      "Epoch [1/10], Step [674/782], Loss: 2.3051, batch time: 6.766114950180054\n",
      "Epoch [1/10], Step [675/782], Loss: 2.3063, batch time: 6.787595987319946\n",
      "Epoch [1/10], Step [676/782], Loss: 2.3124, batch time: 6.749645233154297\n",
      "Epoch [1/10], Step [677/782], Loss: 2.3077, batch time: 6.8327436447143555\n",
      "Epoch [1/10], Step [678/782], Loss: 2.3083, batch time: 6.800379514694214\n",
      "Epoch [1/10], Step [679/782], Loss: 2.3090, batch time: 6.702377557754517\n",
      "Epoch [1/10], Step [680/782], Loss: 2.3091, batch time: 6.787910461425781\n",
      "Epoch [1/10], Step [681/782], Loss: 2.3062, batch time: 6.702369451522827\n",
      "Epoch [1/10], Step [682/782], Loss: 2.3036, batch time: 6.835613250732422\n",
      "Epoch [1/10], Step [683/782], Loss: 2.3060, batch time: 6.709897756576538\n",
      "Epoch [1/10], Step [684/782], Loss: 2.3029, batch time: 6.8080573081970215\n",
      "Epoch [1/10], Step [685/782], Loss: 2.3080, batch time: 6.817663669586182\n",
      "Epoch [1/10], Step [686/782], Loss: 2.2996, batch time: 6.735496759414673\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 15\u001b[0m\n\u001b[1;32m     13\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(outputs, labels)\n\u001b[1;32m     14\u001b[0m \u001b[38;5;66;03m# Backward pass and optimization\u001b[39;00m\n\u001b[0;32m---> 15\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     16\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m     18\u001b[0m \u001b[38;5;66;03m# if (i+1) % 100 == 0:\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/qml/lib/python3.9/site-packages/torch/_tensor.py:488\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    478\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    479\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    480\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    481\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    486\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    487\u001b[0m     )\n\u001b[0;32m--> 488\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    489\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    490\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/qml/lib/python3.9/site-packages/torch/autograd/__init__.py:197\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    192\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    194\u001b[0m \u001b[38;5;66;03m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[1;32m    195\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    196\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 197\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    198\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    199\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(num_epochs):\n",
    "    for i, (images, labels) in enumerate(train_loader):\n",
    "        since_batch = time.time()\n",
    "        \n",
    "        images, labels = images.to(device), labels.to(device)  # Move data to GPU\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(images)\n",
    "\n",
    "        # Compute loss\n",
    "        loss = criterion(outputs, labels)\n",
    "        # Backward pass and optimization\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # if (i+1) % 100 == 0:\n",
    "        print(f\"Epoch [{epoch+1}/{num_epochs}], Step [{i+1}/{len(train_loader)}], Loss: {loss.item():.4f}, batch time: {time.time() - since_batch}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing loop\n",
    "model.eval()\n",
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for images, labels in test_loader:\n",
    "        images, labels = images.to(device), labels.to(device)  # Move data to GPU\n",
    "        outputs = model(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "print(f\"Accuracy on the test set: {(100 * correct / total):.2f}%\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "qml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
